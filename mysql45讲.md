### 本笔记是极客时间丁奇的MySQL45讲笔记整理

# 01基础架构 一条SQL查询语句是如何执行的

![MySQL逻辑架构图](https://github.com/fikery/learnNotes/blob/master/source/0d2070e8f84c4801adbfa03bda1f98d9.png)

大体来说，MySQL可以分为**Server层**和**引擎层**两部分。Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖大多数核心服务功能，及所有的内置函数，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。存储引擎层则负责数据的存储和提取，其架构是插件式的，支持InnoDB、MyISAM等多个存储引擎，现在常用，也是MySQL5.5版本开始的默认引擎InnoDB。

#### 1.连接器

连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接器在跟服务端完成TCP握手后，就开始认证你的身份，如果用户名和密码通过认证，连接器就会到权限表中查询你拥有的权限，之后当前连接里所有权限判断都依赖此时读取的权限。这就意味着一个用户成功建立连接后，即使用管理员账号对此用户做了权限修改，也不会影响已经建立的连接权限，只有再建立新的连接，才会使用新的权限。

客户端如果长时间没动静，连接器就会自动断开，这个时间是参数wait_timeout控制的，默认值是8小时。建立连接的过程比较复杂，因此尽量使用长连接，但是长连接会导致MySQL内存占用涨的快，因为MySQL在执行过程中临时使用的内存是管理在连接对象里的，这些资源在连接断开时才释放。所以长连接累计可能导致内存占用过大，导致MySQL被系统强行杀掉(OOM)，现象就是MySQL异常重启了。

解决长连接资源占用问题，有2种方案可选：

1. 定期断开长连接，使用一段时间，或者程序中判断执行过一个大内存查询后，断开连接，然后重连。

2. 如果是MySQL5.7版本及以上，可以在每次执行一个大操作后，执行mysql_reset_connection来重置连接。这个过程无需重连和权限验证。

#### 2.查询缓存

MySQL拿到一个查询后，会先到查询缓存查看，之前有没有执行过这条语句，如果执行过其结果可能以k(语句)-v(结果)形式存在缓存中，此时如果查询命中缓存，则直接返回结果。但是大多数情况下不要使用查询缓存，因为往往弊大于利。查询缓存的失效非常频繁，只要有更新，表上的所有查询缓存都会被清空。所以除非业务就是个静态表，很久才更新，才适合使用查询缓存。MySQL8.0版本已经将查询缓存的整块功能删掉了。

#### 3.分析器

分析器开始真正执行语句，会做词法分析、语法分析等，分析器会判断语句是否正确，表是否存在，列是否存在等工作。

#### 4.优化器

优化器是在表里有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联(join)时，决定各个表的连接顺序，优化器会选择更高执行效率的方案。

#### 5.执行器

MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就到了执行器阶段，开始执行语句。开启执行的时候，先判断你对这个表是否有操作的权限，如果没有就返回权限错误(如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证)，如果有权限，就打开表继续执行，打开表会调用相应的引擎。

执行器一般会调用引擎接口读取符合条件的“第一行”，判断是否存在结果集中，然后调用引擎接口取“下一行”，直到表的“最后一行”，然后执行器将结果集返回给客户端。数据库的慢查询日志有rows_examined的字段，表示这个语句执行过程中扫描了多少行，这个值是执行器每次调用引擎时累加的。但是在有些场景下，执行器调用一次，引擎却扫描了多行，因此引擎扫描行数和rows_examined并不是完全相同的。

# 02日志系统 一条SQL更新语句是如何执行的

更新流程与查询流程基本一致，但是还涉及两个重要的日志模块，分别是redo log（重做日志）和binglog（归档）日志。

具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，此时更新就算完成了。另外InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里，往往是在系统空闲的时候做。InnoDB的redo log是固定大小的，从头开始写，写到末尾又回到开头循环写。redo log可以保证数据库发生异常重启，之前提交的记录不丢失，这个能力称为crash-safe。类似于收账先写在白板上，下班了再写到账本上，如果白板写满了，就要先把一部分写到账本上，然后再擦掉这部分，继续记录。

1. redo log是InnoDB引擎特有的日志，server层也有自己的日志binlog，即binlog是所有引擎都可以使用的。
2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑。
3. redo log是循环写的，空间固定会用完；binlog是追加写的，不会覆盖之前的日志。

redo log的写入拆成了两个步骤：prepare和commit，这就是“**两阶段提交**”。目的是为了让两份日志之间的逻辑一致。因为假如不采用两阶段提交，比如更新age=0到age=1:

1. 先写redo log再写binlog。假设redo log写完，binlog还没写完，MySQL进程异常重启，这时仍然可以把数据恢复age=1。但是binlog没有记录，因此备份日志的时候就没这条语句，倘若用这个binlog恢复临时库的话，就会少这一次更新，与原库不同。

2. 先写binlog再写redo log。如果写了binlog后发生crash异常，由于redo log没写，崩溃后事务无效，age还是0，但是binlog已经记录了更新日志，所以后面使用binlog恢复的时候就多了一个事务，恢复出来的age就是1，与原库不同。


# 03事务隔离 为什么你改了我还看不见

事务的特性：ACID原则，**原子性、一致性、隔离性、持久性**

多事务同时提交可能导致的问题：**脏读、不可重复读，幻读**

事务隔离级别：**读未提交、读提交、可重复读、串行化**

读未提交：A事务还未提交，但是所做的更改就可以被B事务看到

读提交： A事务提交之后，所做的更改才能被B事务看到

可重复读：事务执行过程中看到的数据保存一致，未提交的更改对其他事务是不可见的

串行化：事务加上读写锁，出现冲突后，后访问的事务需要等前一个事务执行完毕，才能继续执行。

**事务隔离的实现**：每条记录在更新的时候都会同时记录一条回滚操作。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）

回滚日志会在系统认为没有事务需要用到他们的时候删除。什么时候是不需要他们了呢？就是当系统里没有比这个回滚日志更早的read-view的时候。

**为什么尽量不要使用长事务？**长事务意味着系统里面会存在很老的事务视图，在这个事务提交前，回滚记录都要保留，会占用大量存储空间。除此之外，长事务还会占用锁资源，可能会拖垮库。

**事务启动方式**：
1. 显式启动事务语句，begin或者start transaction，提交commit，回滚rollback

2. set autocommit=0，该命令会把这个线程的自动提交关闭，这样只要执行一个select语句，事务就会启动，并不会自动提交，直到主动commit或rollback或断开连接

建议使用方法1，如果考虑多一次性交互问题，可以使用commit work and chain 语法。在autocommit=1的情况下用begin显示启动事务，如果执行commit则提交事务，如果执行commit work and chain则提交事务并自动启动下一个事务。


# 04深入浅出索引a

1. 索引的作用：提高数据查询效率

2. 常见的所有模型：哈希表、有序数组、搜索树

3. 哈希表是一种键值类型数据表，把值放在数组里，用一个函数把key换算成一个确定的位置，然后把value放在数组中这个位置。哈希表可能产生哈希冲突，解决办法是用**链表**存储这些冲突value

4. 哈希表适用等值查询的场景，有序数组适合静态存储场景

5. 有序数组查询效率高，更新效率低

6. 数据库存储不适合二叉树，因为树高太大，采用N叉树。InnoDB中的索引模型是B+Tree

7. 索引类型有主键索引和非主键索引。**主键索引**的叶子节点存的是整行的数据(聚簇索引)，**非主键索引**的叶子节点内容是主键的值(二级索引)

8. 主键索引和非主键索引的区别：主键索引只需要搜索id这个B+Tree即可拿到数据，普通索引要先索引拿到主键，再通过主键索引搜索数据(回表)

9. 一个数据页满了，这时要插入一个数据，按照B+Tree的算法，新增一个数据页，叫做**页分裂**，会导致性能和空间利用率下降。当相邻的两个数据页利用率很低的时候，会做数据页合并，合并的过程是分裂的逆过程。

10. 考虑到性能和存储空间，自增主键往往是更合理的选择。不会触发页分裂现象。


# 05深入浅出索引b

1. 覆盖索引

如果查询条件使用的是普通索引(或是联合索引的最左原则字段)，查询结果是联合索引的字段或是主键，不用回表操作，直接返回结果。

2. 最左前缀

联合索引的最左N个字段，也可以是字符串索引的最左M个字符。

3. 联合索引

根据创建联合索引的顺序，以最左原则进行where检索，比如（age，name）以age=1 或 age= 1 and name=‘张三’可以使用索引，单以name=‘张三’ 不会使用索引，考虑到存储空间的问题，还请根据业务需求，将查找频繁的数据进行靠左创建索引。

4. 索引下推

like 'hello%’and age >= 10 检索，MySQL5.6版本之前，会对匹配的数据进行回表查询。5.6版本后，会先过滤掉age<10的数据，再进行回表查询，减少回表率，提升检索速度


# 06全局锁和表锁 给表加个字段怎么有这么多障碍

根据加锁的范围，MySQL里面的锁可以分为：**全局锁，表级锁，行级锁**

**一、全局锁**

对整个数据库实例加锁

MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL)

整个命令使得整个库处于只读状态，使用该命令后，数据更新语句、数据定义语句和更新类事务提交语句等操作都会被阻塞。

使用场景：全库逻辑备份

风险：

1. 如果在主库备份，备份期间不能更新，业务停摆

2. 如果在从库备份，备份期间不能执行主库同步来的binlog，导致主从延迟

官方自带的逻辑备份工具mysqldump，当其使用参数-single-transaction的时候，会启动一个事务，确保拿到一致性视图(可重复读)，而由于MVCC(多版本并发控制)的支持，这个过程中数据是可以正常更新的。

可重复读(一致性读)虽好，但是前提是引擎要支持这个隔离级别，-single-transaction方法只适用于所有的表使用事务引擎的库，对于MyISAM这种不支持事务的引擎，备份过程中有更新，总是可以拿到最新的数据，就破坏了备份的一致性，此时就需要FTWRL命令了。

既然要全库只读，为什么不用set global readonly=true的方式呢？理由如下：

1. 在有些系统中，readonly的值会被用作其他逻辑，比如判断主备库，因此修改global变量的方式影响太大。

2. 异常处理机制上有差异。如果执行FTWRL命令后由于客户端断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新状态；而设置readonly后，客户端异常，则数据库就会一直保持readonly状态，这会导致整个库处于不可写状态，风险较高。

3.  readonly对super权限无效。

**二、表级锁**

MySQL里面的表级锁有2种，一种是**表锁**，一种是**元数据锁**(meta data lock, MDL)

表锁的语法是：lock tables ... read/write。可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables语法除了限制别的线程对表的读写，也限定了本线程接下来的操作，比如读锁不能写。对于InnoDB这种支持行锁的引擎，一般也不使用表锁来控制并发，毕竟影响还是太大。

MDL不需要显示使用，在访问表的时候自动加上，作用是保证读写的正确性。对一个表做增删改查操作的时候，加MDL读锁；对表结构做变更操作的时候，加MDL写锁。读锁之间不互斥，可以多个线程同时对一张表增删改查；读写锁之间、写锁之间互斥，如果多个线程同时要给一个表增加字段，需要依次执行。MDL直到事务提交才会释放，因此在做表结构变更的时候，一定要小心不要导致锁住线上查询与更新。

# 07行锁功过，怎么减少行锁对性能的影响

1. 行锁是引擎自己实现的，MyISAM引擎不支持行锁，意味着并发控制只能使用表锁，同一张表上同一时刻只能有一个更新在执行。InnoDB支持行锁。

2. **两阶段锁协议**

在InnoDB事务中，行锁是在需要的时候加上，但不是不需要了就立刻释放，而是要等到事务结束时才释放。这个设定意味着，如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

3. 死锁和死锁检测

当事务A在等待事务B释放id=2的行锁，事务B在等待事务A释放id=1的行锁，就进入了死锁状态。出现死锁以后，有两个策略。

* 一种策略是，直接进入等待，直到超时。超时时间可以通过参数innodb_lock_wait_timeout来设置（默认50s）。

* 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑（默认开启）。

如果是大量并发线程要同时更新同一行，那么死锁检测是很高数量级的，虽然最终检测是没有死锁，但是期间消耗了大量CPU资源，结果就是CPU利用率很高，每秒却执行不了几个事务。那么**如何解决这种由热点更新导致的性能问题呢？**

一种有风险的方式是，如果你可以确保这个业务不会出现死锁，那么可以临时关闭死锁检测。但是关闭死锁检测可能会出现大量的超时，业务有损。

另一种方式是控制并发度。这个并发度的控制要放在数据库服务端，而不应该是客户端。因为客户端可能会很多，即使控制每个客户端几个并发线程，但是汇总起来还是很高的并发。在服务端，如果有中间件，可以考虑在中间件实现；如果能够修改MySQL源码，可以对相同行的更新，在进入引擎前排队；如果没有中间件，也没有可以修改源码的专家，那么可以考虑设计优化，将一行改成逻辑上的多行来减少锁冲突。比如更新账户总额，可以放在10条记录上，最后总额是10个记录之和，这样增加总额时，随机选一条记录来加，这样冲突概率就变为之前的1/10。



