### 本笔记是极客时间丁奇的MySQL45讲笔记整理

# 01基础架构 一条SQL查询语句是如何执行的

![MySQL逻辑架构图](https://github.com/fikery/learnNotes/blob/master/source/0d2070e8f84c4801adbfa03bda1f98d9.png)

大体来说，MySQL可以分为**Server层**和**引擎层**两部分。Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖大多数核心服务功能，及所有的内置函数，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。存储引擎层则负责数据的存储和提取，其架构是插件式的，支持InnoDB、MyISAM等多个存储引擎，现在常用，也是MySQL5.5版本开始的默认引擎InnoDB。

#### 1.连接器

连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接器在跟服务端完成TCP握手后，就开始认证你的身份，如果用户名和密码通过认证，连接器就会到权限表中查询你拥有的权限，之后当前连接里所有权限判断都依赖此时读取的权限。这就意味着一个用户成功建立连接后，即使用管理员账号对此用户做了权限修改，也不会影响已经建立的连接权限，只有再建立新的连接，才会使用新的权限。

客户端如果长时间没动静，连接器就会自动断开，这个时间是参数wait_timeout控制的，默认值是8小时。建立连接的过程比较复杂，因此尽量使用长连接，但是长连接会导致MySQL内存占用涨的快，因为MySQL在执行过程中临时使用的内存是管理在连接对象里的，这些资源在连接断开时才释放。所以长连接累计可能导致内存占用过大，导致MySQL被系统强行杀掉(OOM)，现象就是MySQL异常重启了。

解决长连接资源占用问题，有2种方案可选：

1. 定期断开长连接，使用一段时间，或者程序中判断执行过一个大内存查询后，断开连接，然后重连。

2. 如果是MySQL5.7版本及以上，可以在每次执行一个大操作后，执行mysql_reset_connection来重置连接。这个过程无需重连和权限验证。

#### 2.查询缓存

MySQL拿到一个查询后，会先到查询缓存查看，之前有没有执行过这条语句，如果执行过其结果可能以k(语句)-v(结果)形式存在缓存中，此时如果查询命中缓存，则直接返回结果。但是大多数情况下不要使用查询缓存，因为往往弊大于利。查询缓存的失效非常频繁，只要有更新，表上的所有查询缓存都会被清空。所以除非业务就是个静态表，很久才更新，才适合使用查询缓存。MySQL8.0版本已经将查询缓存的整块功能删掉了。

#### 3.分析器

分析器开始真正执行语句，会做词法分析、语法分析等，分析器会判断语句是否正确，表是否存在，列是否存在等工作。

#### 4.优化器

优化器是在表里有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联(join)时，决定各个表的连接顺序，优化器会选择更高执行效率的方案。

#### 5.执行器

MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就到了执行器阶段，开始执行语句。开启执行的时候，先判断你对这个表是否有操作的权限，如果没有就返回权限错误(如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证)，如果有权限，就打开表继续执行，打开表会调用相应的引擎。

执行器一般会调用引擎接口读取符合条件的“第一行”，判断是否存在结果集中，然后调用引擎接口取“下一行”，直到表的“最后一行”，然后执行器将结果集返回给客户端。数据库的慢查询日志有rows_examined的字段，表示这个语句执行过程中扫描了多少行，这个值是执行器每次调用引擎时累加的。但是在有些场景下，执行器调用一次，引擎却扫描了多行，因此引擎扫描行数和rows_examined并不是完全相同的。

# 02日志系统 一条SQL更新语句是如何执行的

更新流程与查询流程基本一致，但是还涉及两个重要的日志模块，分别是redo log（重做日志）和binglog（归档）日志。

具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，此时更新就算完成了。另外InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里，往往是在系统空闲的时候做。InnoDB的redo log是固定大小的，从头开始写，写到末尾又回到开头循环写。redo log可以保证数据库发生异常重启，之前提交的记录不丢失，这个能力称为crash-safe。类似于收账先写在白板上，下班了再写到账本上，如果白板写满了，就要先把一部分写到账本上，然后再擦掉这部分，继续记录。

1. redo log是InnoDB引擎特有的日志，server层也有自己的日志binlog，即binlog是所有引擎都可以使用的。
2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑。
3. redo log是循环写的，空间固定会用完；binlog是追加写的，不会覆盖之前的日志。

redo log的写入拆成了两个步骤：prepare和commit，这就是“**两阶段提交**”。目的是为了让两份日志之间的逻辑一致。因为假如不采用两阶段提交，比如更新age=0到age=1:

1. 先写redo log再写binlog。假设redo log写完，binlog还没写完，MySQL进程异常重启，这时仍然可以把数据恢复age=1。但是binlog没有记录，因此备份日志的时候就没这条语句，倘若用这个binlog恢复临时库的话，就会少这一次更新，与原库不同。

2. 先写binlog再写redo log。如果写了binlog后发生crash异常，由于redo log没写，崩溃后事务无效，age还是0，但是binlog已经记录了更新日志，所以后面使用binlog恢复的时候就多了一个事务，恢复出来的age就是1，与原库不同。


# 03事务隔离 为什么你改了我还看不见

事务的特性：ACID原则，**原子性、一致性、隔离性、持久性**

多事务同时提交可能导致的问题：**脏读、不可重复读，幻读**

事务隔离级别：**读未提交、读提交、可重复读、串行化**

读未提交：A事务还未提交，但是所做的更改就可以被B事务看到

读提交： A事务提交之后，所做的更改才能被B事务看到

可重复读：事务执行过程中看到的数据保存一致，未提交的更改对其他事务是不可见的

串行化：事务加上读写锁，出现冲突后，后访问的事务需要等前一个事务执行完毕，才能继续执行。

**事务隔离的实现**

每条记录在更新的时候都会同时记录一条回滚操作。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）

回滚日志会在系统认为没有事务需要用到他们的时候删除。什么时候是不需要他们了呢？就是当系统里没有比这个回滚日志更早的read-view的时候。

**为什么尽量不要使用长事务？**

长事务意味着系统里面会存在很老的事务视图，在这个事务提交前，回滚记录都要保留，会占用大量存储空间。除此之外，长事务还会占用锁资源，可能会拖垮库。

**事务启动方式**：

1. 显式启动事务语句，begin或者start transaction，提交commit，回滚rollback

2. set autocommit=0，该命令会把这个线程的自动提交关闭，这样只要执行一个select语句，事务就会启动，并不会自动提交，直到主动commit或rollback或断开连接

建议使用方法1，如果考虑多一次性交互问题，可以使用commit work and chain 语法。在autocommit=1的情况下用begin显示启动事务，如果执行commit则提交事务，如果执行commit work and chain则提交事务并自动启动下一个事务。


# 04深入浅出索引a

1. 索引的作用：提高数据查询效率

2. 常见的所有模型：哈希表、有序数组、搜索树

3. 哈希表是一种键值类型数据表，把值放在数组里，用一个函数把key换算成一个确定的位置，然后把value放在数组中这个位置。哈希表可能产生哈希冲突，解决办法是用**链表**存储这些冲突value

4. 哈希表适用等值查询的场景，有序数组适合静态存储场景

5. 有序数组查询效率高，更新效率低

6. 数据库存储不适合二叉树，因为树高太大，采用N叉树。InnoDB中的索引模型是B+Tree

7. 索引类型有主键索引和非主键索引。**主键索引**的叶子节点存的是整行的数据(聚簇索引)，**非主键索引**的叶子节点内容是主键的值(二级索引)

8. 主键索引和非主键索引的区别：主键索引只需要搜索id这个B+Tree即可拿到数据，普通索引要先索引拿到主键，再通过主键索引搜索数据(回表)

9. 一个数据页满了，这时要插入一个数据，按照B+Tree的算法，新增一个数据页，叫做**页分裂**，会导致性能和空间利用率下降。当相邻的两个数据页利用率很低的时候，会做数据页合并，合并的过程是分裂的逆过程。

10. 考虑到性能和存储空间，自增主键往往是更合理的选择。不会触发页分裂现象。

11. 索引有可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引会把数据按照顺序传入，这样页的利用率最高，即索引更紧凑，更省空间。对于重建索引而言，无论是删除主键还是重建主键，都会重建表，可以用一个语句实现：**alter table T engine=InnoDB**


# 05深入浅出索引b

1. 覆盖索引

如果查询条件使用的是普通索引(或是联合索引的最左原则字段)，查询结果是联合索引的字段或是主键，不用回表操作，直接返回结果。

2. 最左前缀

联合索引的最左N个字段，也可以是字符串索引的最左M个字符。

3. 联合索引

根据创建联合索引的顺序，以最左原则进行where检索，比如（age，name）以age=1 或 age= 1 and name=‘张三’可以使用索引，单以name=‘张三’ 不会使用索引，考虑到存储空间的问题，还请根据业务需求，将查找频繁的数据进行靠左创建索引。

4. 索引下推

like 'hello%’and age >= 10 检索，MySQL5.6版本之前，会对匹配的数据进行回表查询。5.6版本后，会先过滤掉age<10的数据，再进行回表查询，减少回表率，提升检索速度


# 06全局锁和表锁 给表加个字段怎么有这么多障碍

根据加锁的范围，MySQL里面的锁可以分为：**全局锁，表级锁，行级锁**

**一、全局锁**

对整个数据库实例加锁

MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL)

整个命令使得整个库处于只读状态，使用该命令后，数据更新语句、数据定义语句和更新类事务提交语句等操作都会被阻塞。

使用场景：全库逻辑备份

风险：

1. 如果在主库备份，备份期间不能更新，业务停摆

2. 如果在从库备份，备份期间不能执行主库同步来的binlog，导致主从延迟

官方自带的逻辑备份工具mysqldump，当其使用参数-single-transaction的时候，会启动一个事务，确保拿到一致性视图(可重复读)，而由于MVCC(多版本并发控制)的支持，这个过程中数据是可以正常更新的。

可重复读(一致性读)虽好，但是前提是引擎要支持这个隔离级别，-single-transaction方法只适用于所有的表使用事务引擎的库，对于MyISAM这种不支持事务的引擎，备份过程中有更新，总是可以拿到最新的数据，就破坏了备份的一致性，此时就需要FTWRL命令了。

既然要全库只读，为什么不用set global readonly=true的方式呢？理由如下：

1. 在有些系统中，readonly的值会被用作其他逻辑，比如判断主备库，因此修改global变量的方式影响太大。

2. 异常处理机制上有差异。如果执行FTWRL命令后由于客户端断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新状态；而设置readonly后，客户端异常，则数据库就会一直保持readonly状态，这会导致整个库处于不可写状态，风险较高。

3.  readonly对super权限无效。

**二、表级锁**

MySQL里面的表级锁有2种，一种是**表锁**，一种是**元数据锁**(meta data lock, MDL)

表锁的语法是：lock tables ... read/write。可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables语法除了限制别的线程对表的读写，也限定了本线程接下来的操作，比如读锁不能写。对于InnoDB这种支持行锁的引擎，一般也不使用表锁来控制并发，毕竟影响还是太大。

MDL不需要显示使用，在访问表的时候自动加上，作用是保证读写的正确性。对一个表做增删改查操作的时候，加MDL读锁；对表结构做变更操作的时候，加MDL写锁。读锁之间不互斥，可以多个线程同时对一张表增删改查；读写锁之间、写锁之间互斥，如果多个线程同时要给一个表增加字段，需要依次执行。MDL直到事务提交才会释放，因此在做表结构变更的时候，一定要小心不要导致锁住线上查询与更新。

# 07行锁功过，怎么减少行锁对性能的影响

1. 行锁是引擎自己实现的，MyISAM引擎不支持行锁，意味着并发控制只能使用表锁，同一张表上同一时刻只能有一个更新在执行。InnoDB支持行锁，但是如果更新的列没有建索引，那么会锁住整个表。

2. **两阶段锁协议**

在InnoDB事务中，行锁是在需要的时候加上，但不是不需要了就立刻释放，而是要等到事务结束时才释放。这个设定意味着，如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

3. 死锁和死锁检测

当事务A在等待事务B释放id=2的行锁，事务B在等待事务A释放id=1的行锁，就进入了死锁状态。出现死锁以后，有两个策略。

* 一种策略是，直接进入等待，直到超时。超时时间可以通过参数innodb_lock_wait_timeout来设置（默认50s）。

* 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑（默认开启）。

如果是大量并发线程要同时更新同一行，那么死锁检测是很高数量级的，虽然最终检测是没有死锁，但是期间消耗了大量CPU资源，结果就是CPU利用率很高，每秒却执行不了几个事务。那么**如何解决这种由热点更新导致的性能问题呢？**

一种有风险的方式是，如果你可以确保这个业务不会出现死锁，那么可以临时关闭死锁检测。但是关闭死锁检测可能会出现大量的超时，业务有损。

另一种方式是控制并发度。这个并发度的控制要放在数据库服务端，而不应该是客户端。因为客户端可能会很多，即使控制每个客户端几个并发线程，但是汇总起来还是很高的并发。在服务端，如果有中间件，可以考虑在中间件实现；如果能够修改MySQL源码，可以对相同行的更新，在进入引擎前排队；如果没有中间件，也没有可以修改源码的专家，那么可以考虑设计优化，将一行改成逻辑上的多行来减少锁冲突。比如更新账户总额，可以放在10条记录上，最后总额是10个记录之和，这样增加总额时，随机选一条记录来加，这样冲突概率就变为之前的1/10。

# 08事务到底是隔离还是不隔离的

1. innodb支持RC和RR隔离级别实现是用的一致性视图

2. 事务在启动时会拍个快照，这个快照基于整个库。基于整个库的意思是，在一个事务内，整个库的修改对于该事务都是不可见的，如果在事务内select表，另外的事务执行了DDL表，根据发生的时间，要么锁住要么报错。

3. 事务是如何实现MVCC呢？

* 每个事务都有一个事务ID，严格递增的transaction id

* 事务启动时，找到已提交的最大事务id，记为up_limit_id

* 事务更新一条语句时，比如id=1改为id=2，会把id=1和该行之前的row_trx_id写到undo log里，并且在数据页上把id的值改为2，把修改这条语句的transaction id记在该行头部

* 一个事务要查看一条数据，要先用该事务的up_limit_id和该行的transaction id对比，如果前者大于等于后者，那么可见；如果前者小于后者，则不可见，只能去undo log里取可见的，当然在undo log里查找的时候，也要作如上对比。

4. 什么是当前读？由于当前读都是先读后写，只能读当前的值。会更新事务内的up_limit_id为该事务的transaction id

5. 为什么RR能实现可重复读，而RC不能？

* 快照读的情况下，RR不能更新事务内的up_limit_id，而RC每次会更新其为快照之前最新已提交事务的transaction id，则RC不能可重复读

* 当前读的情况下，RR是利用record lock+gap lock来实现的，而RC没有gap，所以RC不能可重复读

# 09普通索引和唯一索引如何选择

1. 对于查询过程而言，普通索引，查询到满足条件的第一个记录之后，继续查找下一个记录，直到第一个不满足条件的记录。唯一索引，由于索引唯一性，查到第一个满足条件的记录后，停止查询。二者的性能差距微乎其微，因为InnoDB根据数据页来读写的。

2. 对于更新过程而言，当需要更新一个数据页，如果数据页在内存中就直接更新，如果不在内存中，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存到change buffer中。下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中的相关操作。

3. change buffer是可以持久化的数据，在内存中有拷贝，也会被写到磁盘上。唯一索引的更新插入不能用到change buffer，普通索引的更新插入可以用到。将数据从磁盘读入内存涉及随机IO的访问，是数据库里成本最高的操作之一，change buffer因为减少了随机IO，所以对更新性能的提升很明显。

4. change buffer的使用场景。对于写多读少的业务来说，页面在写完以后立即被访问的概率较小，此时change buffer的效果最好，这种业务模型常见的是账单类、日志类的系统。 相反，如果是写入之后马上做查询，那么反倒会增加性能开销。

5. 业务代码已经保证不会写入重复数据的情况下，尽可能使用普通索引。redo log主要节省的是随机写磁盘的IO消耗，而change buffer主要节省的是随机读磁盘的IO消耗。


# 10MySQL有时候会选错索引

1. 优化器是负责选择索引的，其目的是找到一个最优的执行方案。在数据库里，扫描行数是影响执行代价的因素之一，扫描行数越少，意味着性能越好。当然优化器还会结合是否使用临时表、是否排序等因素综合判断。

2. MySQL在开始执行语句前，也是不知道满足条件的记录有多少条，只能根据统计信息来估计。这个统计信息就是**索引区分度**。一个索引上的不同值越多，其索引区分度越高。这个不同值的个数，称为“基数”，即基数越大，索引区分度越高。

3. 基数是MySQL通过采样统计获得的，因此会不准确。

4. **索引选择异常和处理**

如果遇到这种情况，原本可以执行的很快的SQL语句，实际上却很慢，那么我们有以下3种策略。

* 采用force index() 强行选择索引。缺点是(1)如果索引改了名字，语句也得改，(2)如果迁移数据库，可能语法不兼容。

* 修改语句，引导MySQL使用我们期望的索引。缺点是不通用，有些情况改动语义可能导致其他逻辑问题。

* 新建一个更合适的索引，提供给优化器选择，或者删除误用的索引。

# 11给字符串字段加索引

如何在邮箱这样的字段上建立合理的索引？如果email字段上没有索引，那么email作为条件语句的查询会做全表扫描，由于MySQL支持前缀索引，所以可以**定义字符串的一部分作为索引**。

`alter table SUser add index index1(email);`

`alter table SUser add index index2(email(6));`

index1索引，包含了每条记录的整个字符串，而index2索引，对每个记录只取前6个字节。

**使用前缀索引，定义好长度，就可以做到既节省空间，又不用增加太多查询成本**。那么如何确定前缀长度呢？答案是关注区分度，可以统计索引上有多少个不同的值来判断要使用多长的索引前缀。当然使用前缀索引可能会损失区分度，所以要预设一个可接受的损失比例，比如5%。假设前缀长度在5的时候区分度小于95%，6,7的时候其区分度都在95%以上，那么可以选择前缀长度为6.

**前缀索引可能会增加扫描行数，这会影响到性能，另外前缀索引还会导致覆盖索引无效，这是选择前缀索引需要考虑的因素。**

对于前缀区分度不好的情况，比如身份证号码前面多位都是相对固定的值，该如何建立索引呢？

第一种方式是倒序存储，即把身份证号码倒过来存，那么最后6位就会成为最左前缀，可以有足够的区分度，当然实际中还是需要count(distinct x)验证一下。

第二种方式是hash字段，建立一个整数字段，来保存身份证号码的校验码，同时在这个字段上建立索引(通过crc32()函数进行hash，索引长度4个字节)。不过由于可能存在hash冲突，所以查询语句where部分还要加个条件，判断身份证号码是否精确相同。

**倒序存储和hash字段的异同点**

1. 占用额外空间方面，倒序存储在主键索引上，不会消耗额外存储空间，而hash字段需要增加一个字段，但是考虑到倒序存储4个字节应该不够，再长一些就和hash消耗差不多抵消了。 

2. CPU消耗方面，倒序方式每次读写都要额外调用一次reverse函数，而hash方式需要额外调用一次crc32()函数，从函数计算来看，reverse函数消耗CPU更小些。

3. 查询效率方面，hash方式更稳定一些，虽然有hash冲突的概率，但是概率很小，可以认为平均查询扫描行数为1，而倒序方式还是前缀索引，会增加扫描行数。

# 12为什么我的MySQL会“抖”一下

一条SQL语句，正常执行的时候特别快，但是有时候会变得很慢，这种场景随机且持续时间短，看起来就像是数据库“抖”了一下，why？

当内存数据页跟磁盘数据页内容不一致的时候，我们称内存数据页为“脏页”。内存数据写入磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。平时执行很快的更新操作，其实就是在写内存和日志，而MySQL偶尔“抖”一下，可能就是在刷脏页(flush)

**说明情况会引发数据库flush过程呢？**

1. InnoDB的redo log写满了。此时系统会停止所有更新操作，把checkpoint往前推进，redo log留出空间可以继续写，在checkpoint推进的部分，对应的所有脏页都要flush到磁盘上。然后这部分就成了可以写入的redo log区域。

2. 系统内存不足。当需要新的内存页，但是系统内存不足时，就要淘汰一些数据页，空出内存给别的数据页使用，如果淘汰的是“脏页”，就要flush。这里为什么不是直接把内存淘汰掉，下次请求再从磁盘读入数据页，然后拿redo log来用？其实是基于性能考虑的，flush保证了每个数据页有两种状态：

* 内存里存在，直接返回

* 内存里不存在，读入数据文件到内存，然后返回

这样的效率最高。

3. MySQL认为系统“空闲”的时候。

4. MySQL正常关闭的时候。这时MySQL会把内存的脏页都flush到磁盘上，下次启动MySQL时，就直接从磁盘上读数据，启动速度会很快。

刷脏页虽然是常态，但是以下2种情况，会明显影响性能：

1. 一个查询要淘汰的脏页个数太多，会导致查询响应时间变长。

2. 日志写满，更新阻塞，这种情况对敏感业务来说不能接受。

所以InnoDB需要控制脏页比例，来尽量避免上述2种情况。

**InnoDB刷脏页控制策略**

1. 获取InnoDB所在主机的IO能力，用到innodb_io_capacity这个参数，这个值建议设置成磁盘的IOPS。磁盘的IOPS可以通过fio这个工具，下面的语句可以用来测试磁盘随机读写能力：

`fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest`

2. 在准备刷一个脏页的时候，如果这个数据页旁边刚好是脏页，就会把这个“邻居”一起刷掉，而且这个拖“邻居”下水的逻辑可以蔓延，在innodb中，innodb_flush_neighbors参数就是控制这个行为的，值为1时，就有上述“连坐”机制，值为0时，只刷自己的。找邻居这个策略是机械硬盘时代的意义所在，可以减少很多随机IO，机械硬盘的随机IOPS一般只有几百，相同的逻辑操作减少随机IO就意味着性能的大幅提升。如果使用SSD这种高IOPS的话，建议设置为0，这样可以更快的执行必要的flush操作，减少SQL语句响应时间。

在MySQL8.0中，innodb_flush_neighbors参数的默认值已经是0了。

# 13为什么表数据删除了，表文件大小不变？

首先表数据既可以存在共享表空间里，也可以是单独的文件，这个行为是由参数innodb_file_per_table控制的，OFF表示表数据放在系统共享表空间，即跟数据字典放在一起；ON表示表数据单独存储在一个.idb后缀文件中。从MySQL5.6.6版本开始默认ON，建议也是使用ON设置，这样drop table时可以直接删除文件，回收空间，否则空间不会被回收。

我们删除表的时候，drop table命令回收表空间，但是删除某些行时，表空间并未被回收，为什么呢？

**因为delete命令只是把记录的位置，或者数据页标记为了“可复用”，但是磁盘文件的大小是不变的，即delete命令不能回收表空间，这些可复用，而没有被使用的空间，造成了“空洞”。实际上，不只是删除数据会造成空洞，插入数据也会，如果数据是按照索引递增顺序插入，那么索引是紧凑的，如果数据是随机插入的，就可能造成索引的数据页分裂，从而造成“空洞”。**

也就是说，经过大量的增删改的表，都可能存在空洞，所以如果能够去除空洞，就可以收缩表空间。而重建表，就可以达到这样的目的。使用`alter table A engine=InnoDB`命令来重建表，MySQL会自动完成转存数据、交换表名、删除旧表的操作。在MySQL5.6版本之前，上述命令的DDL过程不是online的，即A不能有更新。在其后的版本引入了online DDL优化了操作流程，允许在重建表的过程中，对A进行增删改操作，当然最好在业务低峰时期进行online DDL。


# 14为什么count(\*)这么慢

统计一个表的行数，`select count(*) from t` 语句可能会随着记录数越来越多而变慢，那么这条语句是怎么实现的呢？

* MyISAM引擎把一个表的总行数存在了磁盘上，因此会直接返回，效率很高

* InnoDB需要把数据一行行的读出来计数，然后返回累计值。

不过这里讨论的都是没有条件where的，否则MyISAM也不能返回这么快。InnoDB之所以不存起来而是读计数，是因为多版本并发控制MVCC的存在，应该返回多少行是不确定的。这与其事务设计有关，可重复读RR是InnoDB的默认隔离级别，在代码上就是通过MVCC实现的。

* MyISAM虽然快，但是不支持事务

* show table status命令虽然快，但是不准确(采样估计的行数)

* InnoDB会遍历全表，结果准确，性能有问题

那么现在有一个页面经常要显示交易系统的操作记录总数，该怎么办呢？答案是，只能我们自己计数，基本思路就是找一个地方，把操作记录表的行数存起来。

1. 用缓存系统保存计数

比如用一个Redis服务保存总行数，虽然速度快，但是不稳定，缓存系统可能会丢失更新，而且在逻辑上并不精确。

2. 在数据库中保存计数

如果把计数直接放到数据库里单独的一张计数表中，首先解决了崩溃丢失的问题，InnoDB支持崩溃恢复不丢数据的，同时因为事务的存在，可以解决计数不精确的问题。因此这是一个较好的方式。

**不同count用法**

关于性能，count(字段) < count(主键id) < count(1) ≈ count(\*)，建议使用count(\*)

* **对于count(主键id)来说**，InnoDB引擎会遍历全表，把每一行的id值都取出来，返回给server层，server层拿到id后，判断是不可能为空的，就按行累加。

* **对于count(1)来说**，InnoDB引擎遍历全表，但不取值，server层对返回的每一行，放一个数字1进去，判断不可能为空的，按行累加。

* **对于count(字段)来说**，如果这个字段定义not null，一行行读出字段并判断不能为null，按行累加；如果字段允许null，那么执行的时候判断可能是null，还要取值判断一下，不是null才累加。

* **count(\*)是例外**，并不会把字段全部取出来，而是专门做了优化，不取值。count(\*)肯定不是null，按行累加。

从并发系统性能的角度考虑，在上述的计数事务序列里，应该先插入操作记录，还是先更新计数表呢？答案是先更新操作记录，再更新计数表，因为更新计数表涉及到行锁的竞争，先插入再更新可以减少事务之间的锁等待，提高并发度。


