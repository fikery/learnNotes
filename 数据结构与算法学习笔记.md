数据结构与算法，是编程的基础知识，同时也是编程能力进阶的核心知识点之一，因此有必要重学数据结构与算法，力求做到熟练使用，可以解决力扣中级难度题的程度

# 1.数组

**数组是一种线性表数据结构，用一组连续的内存空间，存储一组具有相同类型的数据**
* 线性表：数据排成线一样的结构，线性表上的数据只有前、后2个方向。线性表包括数组、链表、队列、栈
* 非线性表：数据间不是简单的前后关系，包括树、图、堆
* 连续的内存空间和相同类型的数据：这保证了数组的**随机访问**特性，但是也导致了**低效的插入和删除**，需要做大量的数据搬移
* 随机访问时间复杂度O(1),插入删除时间复杂度O(n)

计算机会给每个内存单元分配一个地址，通过地址来访问内存中的数据，对一个数组而言，申请一块连续的内存空间，首地址为base_address，计算机访问数组中的某个元素，就是通过数组首地址base_address、该元素的位置i、元素大小data_type_size，计算出该元素的内存地址address = base_address + i * data_type_size

*注意，数组的具有的O(1)时间复杂度特性的是随机访问，而不是查找*

# 2.链表
**链表与数组不同的是，并不需要一块连续的内存空间，而是通过指针将一组分散的内存块串联起来**
* 常见的链表有：单链表、双向链表、循环链表
* 链表除了包含数据外，还存储有指针，正是因为指针的存在，链表的头节点和尾节点比较特殊，访问元素从头节点开始，尾节点的next为null(单链表)
* 随机访问时间复杂度O(n),插入删除时间复杂度O(1)
* 循环链表是一种特殊的单链表，其尾节点指针指向头节点
* 双向链表较为常用，和单链表的结构区别在于支持2个方向，每个节点包含后继指针next和前驱指针prev。

双向链表所需存储空间比单链表大，但是在指定节点的条件下，插入删除都是O(1)时间复杂度，而单链表则需要从头查找是O(n)时间复杂度。另外对于一个有序链表，双向链表按值查询的效率也更高，因为可以存储上次查找的位置p，然后根据目标值与p的大小决定向前还是向后查找

```
1. 力扣第206题，反转链表

输入: 1->2->3->4->5->NULL
输出: 5->4->3->2->1->NULL

# Definition for singly-linked list.
# class ListNode:
#     def __init__(self, x):
#         self.val = x
#         self.next = None

普通解法：
class Solution:
    def reverseList(self, head):
        # 前指针节点
        pre = None
        # 当前指针节点
        cur = head
        # 每次循环，都将当前节点指向它前面的节点，然后前节点和当前节点后移
        while cur:
            # 临时节点，暂存当前节点的下一节点，用于后移
            node = cur.next
            # 将当前节点指向它前面的节点
            cur.next = pre
            # 前指针后移
            pre = cur
            # 当前指针后移
            cur = node
        return pre
        
神仙解法：
class Solution:
    def reverseList(self, head):
        """
        :head: ListNode
        :return: ListNode
        """
        p, rev = head, None
        while p:
            rev, rev.next, p = p, rev, p.next
        return rev
        
2. 力扣第141题，环形链表
给定一个链表，判断链表中是否有环。

class Solution:
    def hasCycle(self, head):
        # 定义慢指针，一次移动1个单位
        i=head
        # 定义快指针，一次移动2个单位
        i=head
        j=i
        while head:
            # 当快慢指针有一个后继节点为None时，说明不是循环链表
            if not i.next or not j.next or not j.next.next:
                return False
            i, j = i.next, j.next.next
            # 当快慢指针相遇时，说明是循环链表
            if i==j:
                return True
        return False

3. 力扣第21题，合并两个有序链表
将两个升序链表合并为一个新的升序链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 

输入：1->2->4, 1->3->4
输出：1->1->2->3->4->4

class Solution:
    def mergeTwoLists(self, l1, l2):
        # 定义l1和l2的指针p1,p2
        p1, p2 = l1, l2
        # 定义一个头节点l3
        l3 = ListNode(None)
        # 定义l3的指针p3
        p3 = l3
        while p1 and p2:
            # 如果p1的值小于等于p2的值，则将p3的next指向p1节点；否则指向p2节点
            if p1.val <= p2.val:
                p3.next = p1
                p1 = p1.next
            else:
                p3.next = p2
                p2 = p2.next
            # p3指针后移
            p3 = p3.next
        # 如果l1和l2有一个未能遍历完成，则p3的next节点直接指向残留节点
        if not p1:
            p3.next = p2
        else:
            p3.next = p1
        return l3.next

4. 力扣第19题，删除链表的倒数第N个节点
给定一个链表，删除链表的倒数第 n 个节点，并且返回链表的头结点。

给定一个链表: 1->2->3->4->5, 和 n = 2.
当删除了倒数第二个节点后，链表变为 1->2->3->5.

普通解法：
class Solution:
    def removeNthFromEnd(self, head, n):
        # 通过列表l将遍历的链表值存储，最后再移除目标值重新构建链表
        p, l = head, []
        node = ListNode(None)
        cur = node
        while p:
            l.append(p.val)
            p = p.next
        for idx, data in enumerate(l):
            if idx == len(l)-n:
                continue
            cur.next = ListNode(data)
            cur = cur.next
        return node.next

快慢指针解法：
class Solution:
    def removeNthFromEnd(self, head, n):
        a = head
        b = head
        # a指针作为快指针，先走n步
        for i in range(n):
            if a.next:
                a = a.next
            else:
                return head.next
        # 当快指针走到最后的时候，慢指针走到倒数第n个节点
        while a.next:
            a = a.next
            b = b.next
        b.next = b.next.next
        return head
        
5. 力扣第876题，链表中的中间节点
给定一个带有头结点 head 的非空单链表，返回链表的中间结点。
如果有两个中间结点，则返回第二个中间结点。

输入：[1,2,3,4,5]
输出：此列表中的结点 3 (序列化形式：[3,4,5])

class Solution:
    def middleNode(self, head):
        # 快慢指针的思想，p1每次移动一个单位，p2每次移动2个单位，当p2移动到最后的时候，p1位置是中间节点
        p1 = head
        p2 = head
        while p2 and p2.next:
            p1 = p1.next
            p2 = p2.next
            if p2:
                p2 = p2.next
            else:
                break
        return p1

```
# 3.栈
**栈是一种“操作受限”的线性表，只允许在一端进行插入和删除，FILO后进先出的数据结构**
* 栈既可以通过数组实现（顺序栈），也可以通过链表实现（链式栈）
* 栈操作包括入栈、出栈
* 空间复杂度和时间复杂度都是O(1)

**栈在函数调用中的应用**

操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈

**栈在表达式求值中的应用**

编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；

当遇到运算符，就与运算符栈的栈顶元素进行比较。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较

**栈在括号匹配中的应用**

我们假设表达式中只包含三种括号，圆括号 ()、方括号 [] 和花括号{}，并且它们可以任意嵌套。比如，{[{}]}或 [{()}([])] 等都为合法格式，而{[}()] 或 [({)] 为不合法的格式。那我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢？

这里也可以用栈来解决。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。

当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。

```
1. 力扣第20题，有效的括号
给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串，判断字符串是否有效。
有效字符串需满足：
左括号必须用相同类型的右括号闭合。
左括号必须以正确的顺序闭合。
注意空字符串可被认为是有效字符串。

输入: "()"
输出: true

普通解法：
class Solution:
    def isValid(self, s: str) -> bool:
        stack = []
        for i in s:
            if not stack:
                stack.append(i)
                continue
            if i ==')' and stack[-1] == '(' or i ==']' and stack[-1] == '[' or i =='}' and stack[-1] == '{':
                stack.pop()
            else:
                stack.append(i)
        return False if stack else True

大神解法：
class Solution:
    def isValid(self, s):
        while '{}' in s or '()' in s or '[]' in s:
            s = s.replace('{}', '')
            s = s.replace('[]', '')
            s = s.replace('()', '')
        return s == ''


2. 力扣第155题，
3. 力扣第232题，
4. 力扣第844题，
5. 力扣第224题，
6. 力扣第682题，
7. 力扣第496题，

```

# 4.队列

问：**当我们向固定大小的线程池中请求一个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略又是怎么实现的呢？**
答：第一种是非阻塞的处理方式，直接拒绝请求；另一种是阻塞式的处理方式，将请求排队，等有空闲线程时，取出排队的请求继续处理。那么我们需要一个队列来存储请求，基于数组和链表的队列实现方式，有什么区别？基于链表的队列，可以实现一个支持无限排队的无界队列，但是可能导致过多的请求排队，响应时间过长。所以响应敏感的系统，不适合链式无限队列。基于数组的队列，大小有限制，超限后拒绝新的请求，对响应敏感的系统更合理。**实际上对于大部分资源有限的场景，都可以通过队列实现排队来应对资源不足**

队列，先进先出FIFO的数据结构，和栈一样也是一种操作受限的线性表结构。基本操作有：
* 入队，放一个数据到队尾
* 出队，从队头取一个元素

队列也可以分为，顺序队列、链式队列、循环队列。对于顺序队列，数组实现需要2个指针，一个是head指针指向队头，应该是tail指针指向队尾，当head==tail时队空，当tail==n时队满。但是循环队列不同，队空是head==tail，队满是(tail+1)%n=head，且循环队列队满的时候，tail指向的位置没有存储数据，因此会浪费一个数组的存储空间。


# 5.递归

递归是一种应用非常广泛的编程思想，很多数据结构和算法都用到递归的思想，比如DFS深度优先搜索、前中后序遍历二叉树等。

什么样的问题可以用递归的思想来解决呢？
* 一个问题的解可以分为几个子问题的解
* 这个问题与子问题，除了数据规模不同，求解思路完全一样
* 存在递归终止条件

编写递归代码最重要的是**写出递归公式，找到终止条件**。总的来说，**写递归代码的关键就是找到将大问题分解为小问题的规律，并基于此写出递推公式，然后找出终止条件，最后将公式和条件转化为代码**

递归代码要注意**堆栈溢出**，避免过度深入的递归。通常递归会比循环速度慢的原因包括**递归有大量的重复计算**和每次递归调用都会在内存栈中保存一次现场数据，

# 6.排序

问：**如何在O(n)的时间复杂度内查找一个无序数组中的第K大元素？**

答：快排的核心思想是分治与分区，利用原地分区策略和二分策略，取数组最后一个元素作为pivot，对数组原地分区，如果大于pivot的数据放在左边，小于pivot放在右边，这样如果pivot的索引p+1=K，则pivot就是第K大元素；如果p+1<K，则在右边区间继续查找，否则在左区间查找。

排序通常是我们学的第一个算法，经典且常用的排序算法有：
* 冒泡排序
* 插入排序
* 选择排序
* 归并排序
* 快速排序
* 桶排序
* 基数排序
* 计数排序

排序算法 | 时间复杂度 |  是否基于比较  
-|-|-
冒泡，插入，选择 | O(n^2) | 是 |
快排，归并 | O(nlogn) | 是
桶，基数，计数 | O(n) | 否

分析排序，不仅要分析时间复杂度、空间复杂度，还要考虑是否原地排序、是否稳定排序。

排序算法 | 原地排序 |  稳定排序 | 时间复杂度(最好、最坏、平均)  
-|-|-|-
冒泡 | 是 | 是 | O(n)、O(n^2)、 O(n^2)
插入 | 是 | 是 | O(n)、O(n^2)、 O(n^2)
选择 | 是 | 否 | O(n^2)、O(n^2)、 O(n^2)

**尽管插入排序和冒泡排序复杂度相同，为什么实践中我们更倾向于使用插入排序呢？**
实际上从代码来看，由于Python支持交替赋值，所以是一次赋值语句，而其他语言可能不支持这样的表达式，因此冒泡排序需要3个赋值操作，而插入排序只需要1个，因此冒泡排序代码执行效率没有插入排序高。并且冒泡排序的交换次数是固定的，是原始数据的逆序度，而插入排序的有优化的空间，比如希尔排序。

### 1.冒泡排序 ###

```
def bubble_sort(lst):
    """
    冒泡排序，原地排序无需返回新数组
    :param lst:
    :return:
    """
    if len(lst) <= 1:
        return
    n = len(lst)
    for i in range(n):
        # 整个数组有序标记
        flg = False
        for j in range(n-i-1):
            if lst[j] > lst[j + 1]:
                lst[j], lst[j + 1] = lst[j + 1], lst[j]
                flg = True
        if flg is False:
            # 如果一轮循环没有数据交换，则说明已经有序
            break
```

### 2.插入排序 ###

```
def insert_sort(lst):
    """
    插入排序
    :param lst:
    :return:
    """
    if len(lst) <= 1:
        return
    n = len(lst)
    for i in range(1, n):
        # 当前待插入的值
        value = lst[i]
        # 当前要与待插入值比较的索引
        j = i - 1
        # 从后向前比较
        while j >= 0 and value < lst[j]:
            # 如果当前比较的值比待插入待值大，则比较值后移一位
            lst[j + 1] = lst[j]
            j -= 1
        # 最后一个完成排序的比较值，后一位就是待插入值
        lst[j + 1] = value
```

### 3.选择排序 ###

```
def choose_sort(lst):
    """
    选择排序
    :param lst:
    :return:
    """
    if len(lst) <= 1:
        return
    n = len(lst)
    for i in range(n - 1):
        # 默认每层循环，首位是最小索引
        m_idx = i
        for j in range(i + 1, n):
            if lst[j] < lst[m_idx]:
                # 找到最小索引
                m_idx = j
        # 一层循环后，交换首位与最小位的值
        lst[i], lst[m_idx] = lst[m_idx], lst[i]
```

冒泡、插入、选择排序，时间复杂度高，适合小规模数据的排序，对于大规模数据，采用时间复杂度O(nlogn)的归并排序和快速排序更适合。归并和快排都利用了分治的思想，即将一个大问题分解为小问题来解决，小问题解决了，大问题自然就解决了。分治算法一般都是通过递归来实现。

### 4.归并排序 ###

先把数组从中间分为前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并起来，这样整个数组就有序了。归并排序是稳定排序，但是不是原地排序，需要额外申请内存空间存储结果。**其时间复杂度是O(nlogn)，空间复杂度是O(n)**。

分析归并排序，其递推公式是`merge_sort(p...r)=merge(merge_sort(p...q), merge_sort(q+1,r))`,终止条件是`p>=r,不用继续分解`。归并排序代码如下：
```
def merge_sort(lst):
    """
    归并排序
    :param lst:
    :return:
    """
    n = len(lst)
    if n <= 1:
        return lst
    # 把数组分为两部分
    mid_idx = n // 2
    l, r = lst[:mid_idx], lst[mid_idx:]
    # 对每部分对数组再进行排序
    l_res = merge_sort(l)
    r_res = merge_sort(r)
    # 合并两个数组
    res_list = []
    while l_res and r_res:
        # 利用list特性，POP出首位最小的数
        res_list.append(l_res.pop(0)) if l_res[0] <= r_res[0] else res_list.append(r_res.pop(0))
    res_list.extend(l_res) if not r_res else res_list.extend(r_res)
    return res_list
```

### 5.快速排序 ###

快排利用的也是分治思想，乍一看和归并相似，但是思路其实完全不一样。简单来说，快排的思想是，给定一个数组，选择一个分区点pivot，然后遍历数组，将小于pivot的放到左边，大于pivot的放到右边。然后循环对左右两边的数据继续分区，直到区间缩小为1，则所有数据都有序了。快排原地排序，需要用到交换操作，因此不是稳定排序。递推公式是`quick_sort(p...r)=quick_sort(p...q-1)+quick_sort(q+1,r)`,终止条件是`p>=r`。代码如下：
```
def quick_sort_simple(lst):
    """
    快速排序，非原地排序，稳定排序
    :param lst:
    :return:
    """
    if len(lst) <= 1:
        return lst
    pivot = lst[-1]
    low = [x for x in lst[:-1] if x <= pivot]
    high = [x for x in lst[:-1] if x > pivot]
    low_res = quick_sort_simple(low)
    high_res = quick_sort_simple(high)
    return low_res + [pivot] + high_res

def quick_sort(lst, l=None, r=None):
    """
    快速排序，原地排序，非稳定排序
    :param lst:
    :return:
    """
    if len(lst) <= 1:
        return
    if l is None and r is None:
        l, r = 0, len(lst) - 1
    if l >= r:
        return
    # 这里的原地分区思想是，pivot分割出左右两部分，[l,idx-1]小于pivot，[idx+1,r]大于pivot
    idx, pivot = l, lst[r]
    for i in range(l, r):
        if lst[i] < pivot:
            # 如果当前值小于pivot，则交换idx位置的值，同时idx向右前进一位
            lst[i], lst[idx] = lst[idx], lst[i]
            idx += 1
    # 交换idx和pivot，让pivot的值成为左边部分的上限
    lst[r], lst[idx] = lst[idx], lst[r]
    # 完成原地分区，处理左右分区
    quick_sort(lst, l, idx - 1)
    quick_sort(lst, idx + 1, r)
```

归并排序是自下而上，先处理子问题，再合并；而快排则是自上而下，先分区，再处理子问题。归并排序无法做到原地排序，因此在大数据场景下，时间复杂度为O(nlogn)、空间复杂度为O(1)的快排更有优势。


# 7.线性排序

问：**如何根据年龄给100万用户数据排序？**
答：计数排序的思想，假设年龄范围是1～120岁，那么可以将用户划分到120个桶内，然后依次遍历120个桶内元素，这样就得到了100万用户的年龄排序结果。

线性排序指的是时间复杂度是O(n)的排序算法：**桶排序、计数排序、基数排序**。之所以这3个算法可以做到线性排序，是因为它们都不是基于比较的排序算法。但是线性排序对数据的要求很苛刻，因此其适用场景需要特别注意。

### 1.桶排序 ###

核心思想是把待排序的数据分到几个**有序的桶**里，每个**桶内再分别进行排序**。这样等桶内排序完成后，按顺序取出不同桶的数据，整体就是有序的了。桶排序的条件如下：
* 待排序数据可以容易划分成m个桶，且桶间有序。这样桶内数据排序后，桶间无序排序
* 桶内数据分布均匀。如果桶内数据多多少少不均匀，极端情况下，所有的数据都在一个桶里，时间时间复杂度就退化到O(nlogn)了
* 桶排序适合用在外部排序中。即排序数据存储在外部磁盘中，数据量大，无法全部加载到内存中。

**比如我们对10GB的数据进行排序(可能是订单金额数据)，但是内存有限只有2GB，那么如何做呢？**我们先假定金额均匀分布，那么可以先扫描一遍文件，记录金额最大最小值，比如最大10万最小1元，那么我们可以划分出10个桶，第1个桶存1～1万元的数据，第2个桶存10001～2万元的数据，以此类推。每个桶对应一个文件，那么对单个桶进行快排。完成桶内快排后，只需依次读取这10个文件，追加写入到1个文件中，即可完成大文件排序。那么假如金额不是均匀分布的呢？我们可以针对划分后的大桶，继续进行划分，直到所有的文件都可以读入内存。

### 2.计数排序###

当要排序的n个数据，范围k不大的时候，可以把数据分为k个桶，每个桶内数据都相同，无需排序，只需桶间排序即可。**比如高考分数查询系统，如何查询某个考生在省内的排名呢？**高考成绩750分，对一个省的50万考生来说，都处在这个分数内，因此可以将50万考生划分到750个桶内，这样只需要依次扫描每个桶，输出到一个数组中，就完成了50万考生的排序。

计数排序的思想简单，但是算法还是需要巧妙构思的。理解起来也不是很容易，这里就不做具体介绍了。总的来说，**计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数，比如全员乘以10或者加上10**

### 3.基数排序 ###

**如何对10万个手机号码进行排序？**

手机号码有个特点，如果前面几位a比b大，那后面的无需再看。借助稳定排序算法，有个巧妙的实现思路。**按照高低位从后向前按位排序**，这样经过11次排序后，号码就都有序了。需要注意的是，按位排序必须是稳定的，否则排到后面高位的时候，低位的顺序乱了，就没意义了。而针对每个位进行排序，可以采用桶排序或者计数排序。手机号11位且每一位范围都不超过10，那么可以进行11次计数排序。进行k次桶/计数排序，总的时间复杂度是O(k\*n)，当k不大的时候，基数排序的时间复杂度就近似于O(n)。

总的来说，**基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到 O(n) 了**


# 8.排序优化

# 9.二分查找

问：**如何快速定位出一个 IP 地址的归属地？**

答：由于IP归属地是一个段，需要判定单个IP落在哪个IP段内。假如有12万条IP区间与归属地段对应数据，如何快速定位某个IP段归属地呢？
可以先预处理这12万条数据，让其按照起始IP从小到大排序（起始IP地址可以转化为32位整型数字），然后问题就变成了**在有序数组中，查找最后一个小于等于给定值的元素**了，即二分查找的第4种变形问题。

**二分查找针对的是一个有序数据集合，查找思想类似分治思想。每次通过跟区间的中间元素对比，将查找区间缩小为之前的一半，直到找到要查找的元素，或者区间缩小为0**

二分查找是一种高效的查找算法，时间复杂度是O(logn)，和堆、二叉树的时间复杂度一样。简单二分查找不包括重复数据，变形二分查找会有重复数据。但是二分查找的局限性也很明显：
* 二分查找依赖顺序表结构，即数组
* 二分查找针对有序数据，否则需要先排序，排序的最低时间复杂度是O(nlogn)
* 数据量太小没必要用二分查找，顺序遍历即可；数据量太大不适合二分查找，底层数组内存空间限制

虽然大部分情况下，用二分查找可以解决的问题，也可以用哈希表、二叉树解决，但是后者需要额外内存空间，二分查找底层依赖的是数组，不需要额外存储空间，是最省内存的方式。

```
# 二分查找的循环实现
def bsearch(lst, value):
    """
    简单二分查找
    :param lst:  传入数组
    :param value: 查找值
    :return:
    """
    # 端点索引
    low, high = 0, len(lst)-1
    while low <= high:
        # 中间值，位操作，防止溢出加快速度
        mid = low + ((high - low) >> 1)
        if value == lst[mid]:
            return mid
        if value > mid:
            low = mid + 1
        else:
            high = mid - 1
    return -1

# 二分查找的递归实现
def bsearch(lst, value):
    """
    简单二分查找
    :param lst:  传入数组
    :param value: 查找值
    :return:
    """
    # 端点索引
    low, high = 0, len(lst) - 1
    # 递归函数
    def inner(lst, low, high, value):
        if low > high:
            return -1
        mid = low + ((high - low) >> 1)
        if lst[mid] == value:
            return mid
        if value > mid:
            return inner(lst, mid + 1, high, value)
        else:
            return inner(lst, low, mid - 1, value)

    return inner(lst, low, high, value)

```

尽管第一个二分查找算法于1946年出现，而第一个完全正确的二分查找算法实现，直到1962年才出现。变形二分查找有4种常见类型：
* 查找第一个值等于目标值的元素，如\[1,2,3,4,4,5]，查找第一个4
```
def advanced_bsearch(lst, value):
    """
    变形二分查找，查找第一个值等于目标值的元素
    :param lst: [1,2,3,4,4,5]
    :param value: 4
    :return: 3
    """
    low, high = 0, len(lst) - 1
    while low <= high:
        mid = low + ((high - low) >> 1)
        if value > lst[mid]:
            low = mid + 1
        elif value<lst[mid]:
            high = mid - 1
        else:
            if mid==0 or lst[mid-1] != value:
                return mid
            else:
                high = mid-1
    return -1
```

* 查找最后一个值等于目标值的元素，如\[1,2,4,4,5]，查找最后一个4
```
def advanced_bsearch(lst, value):
    """
    变形二分查找，查找最后一个值等于目标值的元素
    :param lst: [1,2,4,4,5]
    :param value: 4
    :return: 3
    """
    low, high = 0, len(lst) - 1
    while low <= high:
        mid = low + ((high - low) >> 1)
        if value > lst[mid]:
            low = mid + 1
        elif value<lst[mid]:
            high = mid - 1
        else:
            if mid==n-1 or lst[mid+1] != value:
                return mid
            else:
                low = mid+1
    return -1

```

* 查找第一个大于等于目标值的元素，如\[1,2,3,6,7,8]，查找第一个大于等于4
```
def advanced_bsearch(lst, value):
    """
    变形二分查找，查找第一个值大于等于目标值的元素
    :param lst: [1,2,3,6,7,8]
    :param value: 4
    :return: 3
    """
    low, high = 0, len(lst) - 1
    while low <= high:
        mid = low + ((high - low) >> 1)
        if value > lst[mid]:
            low = mid + 1
        else:
            if mid == 0 or lst[mid - 1] < value:
                return mid
            else:
                high = mid - 1
    return -1
```

* 查找最后一个小于等于目标值的元素，如\[1,2,4,4,5]，查找第一个4
```
def advanced_bsearch(lst, value):
    """
    变形二分查找，查找最后一个小于等于目标值的元素
    :param lst: [1,2,4,4,5]
    :param value: 4
    :return: 3
    """
    low, high = 0, len(lst) - 1
    while low <= high:
        mid = low + ((high - low) >> 1)
        if value < lst[mid]:
            low = mid + 1
        else:
            if mid == n-1 or lst[mid + 1] > value:
                return mid
            else:
                low = mid + 1
    return -1
```

```
力扣第33题，搜索旋转排序数组

输入: nums = [4,5,6,7,0,1,2], target = 0
输出: 4

class Solution:
    def search(self, nums: List[int], target: int) -> int:
        if not nums:
            return -1
        # 二分查找
        low,high = 0,len(nums)-1
        while low<=high:
            # 中间值
            mid = low+((high-low)>>1)
            if nums[mid] == target:
                return mid
            if nums[mid]<nums[high]:
                # 则nums[mid:high]是有序的
                if nums[mid] < target <= nums[high]:
                    low = mid+1
                else:
                    high = mid-1
            else:
                # nums[low,mid]是有序的
                if nums[low]<=target<nums[mid]:
                    high = mid-1
                else:
                    low = mid+1
        return -1
```


# 10.跳跃表

问，**Redis为什么采用跳表实现有序集合，而非红黑树？**

答：严格来说，Redis中的有序集合通过跳表和哈希表实现的。由于有序集合的核心操作包括插入、删除、查找、区间查找、迭代输出等，其中插入、删除、查找及迭代输出这几个操作，红黑树也可以完成，时间复杂度也和跳表一样。但是按照区间查询数据，红黑树效率不如跳表高。当然，跳表实现起来更加简单，且更加灵活，可以通过改变索引构建策略，调整效率和内存消耗。

跳跃表是一种可以支持快速插入、删除、操作操作的动态数据结构，Redis中的有序集合就是用跳表实现的。对于一个单链表来说，即使存储的数据有序，查找也需要从头遍历，时间复杂度就是O(n)；但是如果给链表建立多级索引，比如每两个节点提取一个节点到上一级，抽出来的这级叫做索引层，其中索引节点有个down指针，指向下一级节点。这种**链表加多级索引的结构，就是跳跃表**。跳表的第k级索引的节点个数是第k-1级索引的节点个数1/2，即具有n个节点的单链表，第1级索引节点个数是n/2，第k级索引节点个数是n/(2^k)。采用2分法抽节点，则每级最多访问3个节点。**跳表查询的时间复杂度是O(logn)**，与二分查找一样。这种查询效率的提升，代价就是建立了多级索引，以空间换时间的策略，其**空间复杂度是O(n)**。如果想减少索引空间占用，则可以采用3分法抽节点的策略。

跳表的动态**插入、删除查找，时间复杂度也是O(logn)**，需要注意的是，删除操作时，如果该节点在索引中也有出现，则除了要删除链表上的节点，还要删除索引中的节点

**跳表索引动态更新**，当不停地向跳表中插入数据，就需要更新索引，防止2个索引节点中数据过多的情况。跳表通过随机函数来维护索引与原始链表的平衡。这个随机函数，决定将这个节点插入到哪几级索引中，比如随机函数生成值2，则将该节点添加到第1级和第2级索引中。

# 11.哈希表

问，**如何设计一个工业级的哈希表？**

答：首先要有几点要求：
* 支持快速的查询、插入、删除操作
* 内存占用合理，不能浪费过多的内存空间
* 性能稳定，极端情况下性能也不会退化到无法接受的情况

设计思路从3方面考虑：
* 设计一个合适的哈希函数
* 选择合适的装填因子阈值，设计动态扩容策略
* 选择合适的哈希冲突解决方法

### 1.哈希思想

**哈希表用的是数组支持按照下标随机访问时间复杂度为O(1)的特性，所以在某种程度上，哈希表就是数组的一种拓展**。通过哈希函数把元素的键映射为下标，然后将数据存储在数组对应下标的位置，当按照键取数据时，就是用同样的哈希函数，将键转化为数组下标，然后取数据。

### 2.哈希函数

**哈希函数设计的3点要求**
* 哈希函数计算得到的哈希值是一个非负整数
* 如果key1=key2，则hash(key1)==hash(key2)
* 如果key1!=key2，则hash(key1)!=hash(key2)

其中第三点，实际上几乎不可能实现，哈希冲突难以避免，而且数组存储空间有限，更会增加哈希冲突的概率。哈希函数的设计不能太复杂，否则会影响哈希性能，其次生成的值要尽可能均匀分布，这样可以减少哈希冲突，即使冲突了，也不会导致某个槽数据特别多的情况。

哈希函数的设计方法，有**数据分析法**，比如取手机号后四位作为哈希值来区分手机号；直接寻址法、平方取中法、折叠法、随机数法等等。

### 3.哈希冲突

常用2类方法解决哈希冲突，开放寻址法和链表法

#### 开放寻址法 ####

核心思想是出现了哈希冲突，则重新探测一个空闲位置插入数据。探测位置的方法，比如**线性探测**，如果往哈希表中插数据，该数据被哈希后，存储位置已经被占用了，则从此位置开始，依次向后查找，到底了再从头开始找，直到有空位置，插入哈希值。

在哈希表中查找元素类似插入排序过程，通过哈希函数得到元素键的哈希值，然后比较数组中下标为哈希值的元素和要查找的元素，如果相等明显就是目标元素，否则就按顺序向后依次查找，如果遍历到数组中的空闲位置还没有找到，则判定目标元素不在哈希表中。

哈希表支持插入、查找，也支持删除操作。对于线性探测法解决冲突的哈希表，需要做软删除，即标记删除，否则删除后出现空位，会导致插入冲突时查找位置失效。

线性探测法有很大的问题，就是哈希表中数据越多，哈希冲突概率越大，空闲位置越少，探测时间越久，极端情况下需要探测整个表，时间复杂度退化为O(n)。除此之外，还有**二次探测**和**双重哈希**两种经典探测方法。二次探测相比线性探测，将线性探测的步长由1调整为二次方，原来是hash(key)+0,hash(key)+1,hash(key)+2，现在是hash(key)+0，hash(key)+1^2，hash(key)+2^2。而双重哈希，就是使用一组哈希函数，先用hash1(key)，如果存储位置被占了，则用hash2(key)，以此类推，直到找到空闲存储位置。

**装填因子**=哈希表中的元素个数 / 哈希表长度，装填因子越大，说明空闲位置越少，冲突概率越高，哈希表性能越低。如果装填因子过大，则需要动态扩容，重新申请一个更大的哈希表，然后将数据搬移到新表中。不过哈希表扩容的数据搬移操作比较复杂，因为哈希表大小变了，数据存储位置也变了，因此需要重新计算数据的哈希值（存储位置）。插入一个数据，不扩容的情况下，时间复杂度是O(1)，扩容情况下，时间复杂度是O(n)，均摊情况下，时间复杂度还是O(1)。

装填因子的选择需要权衡，装填因子过大，会导致哈希冲突概率变大，如果过小，会导致内存浪费严重。

**如何避免低效扩容？**避免扩容时一次性搬移数据过多导致耗时太大的情况，可以将扩容操作穿插在插入操作中，分批完成。当装填因子达到阈值触发扩容时，只申请新表空间，不搬移数据，等插入数据时直接插到新表中，然后从老表取一个数据搬移到新表，如此重复操作，即可消解扩容耗时问题。对于查询操作，先从新表查，没有再去老表查。这种方式让插入一个数据的数据复杂度稳定在了O(1)。



#### 链表法 ####

链表法更加简单，也更加常用。在哈希表中，每个数组单元槽对应一个链表，哈希值相同的元素就放在相同槽位对应的链表中。插入的时候，通过哈希函数算出哈希值，即对应的槽位，然后将数据插入对应链表中，所以插入的时间复杂度O(1)，而查找和删除的时间复杂度，与链表长度k成正比，对于哈希均匀的哈希函数而言，理论上k=n/m，n是哈希表中数据个数，m是哈希表中槽的个数.

通过上面的说明，可以知道，哈希表的查询效率并不能笼统的说是O(1)，跟哈希函数、装填因子、哈希冲突都有关系。极端情况下，攻击者通过精心构造的数据，使得所有的数据经过哈希后，都散列到同一个槽里，如果我们采用链表的冲突解决方法，此时哈希表就退化为链表，查询时间复杂度退化为O(n)，加入表中有10万数据，之前查询100次需要0.1秒，如今需要1万秒。这样就可以因为查询消耗系统资源，导致系统无法响应其他请求，从而达到DoS的目的，这也是**哈希表碰撞攻击的基本原理**。


**当数据量较小、装填因子小的时候，适合开放寻址法，这也是Java中ThreadLocalMap使用的开放寻址法的原因。链表法适合存储大对象、大数据量，且更加灵活，比如可以用红黑树替代链表**

# 12.哈希算法

问，**如何防止数据库中的用户信息被脱库？**

答：通过哈希算法，对用户密码进行哈希加密后存储。不过尽管如此，仍然可能通过字典攻击/彩虹表来撞库，此时我们可以通过加盐的方式，增加密码复杂度，拿salt+password的组合进行哈希加密，再存到数据库中，进一步增加破解的难度。或者通过计算哈希足够慢的算法如PBKDF2WithHmacSHA1，降低硬件计算速度，减少安全风险。

问，**哈希算法在分布式系统中有哪些应用？**

答：负载均衡、数据分片、分布式存储

### 1.哈希算法 ###

将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，映射得到的值就是哈希值。

### 2.哈希算法4点要求 ###

* a 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）
* b 对原始数据敏感，即使只修改了一个Bit，得到的哈希值也大不相同
* c 哈希冲突概率要小，不同的原始数据，哈希值相同的概率很小
* d 哈希算法效率要高，长文本也能快速计算哈希值

### 3.常见的哈希算法 ###

MD5(消息摘要算法)，SHA(安全哈希算法)，DES(数据加密标准)，AES(高级加密标准)等

### 4.哈希算法的应用 ###

#### (1)安全加密 ####

采用的安全加密算法有MD5和SHA，对于一个哈希算法，我们能做的是尽量减少哈希碰撞的概率，因为哈希算法产生的哈希值长度是有限的，而要哈希的数据则是无限的，因此根据抽屉原理，必有冲突的情况。所以实际上，足够低的冲突概率可以满足加密要求，如MD5有128位，则对应2^128个不同的哈希值，即冲突概率小于1/2^128。越复杂的加密算法，需要的计算时间也越长，因此选择加密算法，需要衡量破解难度与计算时间。

#### (2)唯一标识 ####

对于存储海量图片来说，通过哈希算法生成唯一图片名称是一个常见方式。比如可以从图片的二进制码串中，取开头100个字节，中间100个中间，最后100个字节，然后将这300个字节进行哈希操作，得到哈希值作为图片唯一标识。

#### (3)数据校验 ####

比如通过迅雷下载文件，基于P2P协议的方式，会从多个机器上并行下载文件块，全部下载完成后再组装成完整的文件。由于网络传输并不安全，下载的文件块可能被恶意修改过，因此需要校验文件块的安全、正确、完整。方法可以是，对100个文件块分别取哈希值，存在种子文件中，当全部下载完成，再通过相同的哈希算法对下载的文件逐一哈希，最后跟种子文件中保存的哈希值对比，进行数据校验。

#### (4)哈希函数 ####

哈希函数是设计一个哈希表的关键，它决定了哈希冲突的概率和哈希表的性能。不过相对于其他应用，哈希函数对冲突的要求要低很多，毕竟可以通过**开放寻址法**或者**链表法**等方式可以解决。另一方面，哈希函数对能否反向解密也不关心，更加关注的是哈希值能否均匀分布。因此哈希函数用的算法一般比较简单，追求效率。

#### (5)负载均衡 ####

负载均衡的算法有轮询、随机、加权轮询等，那**如何实现一个会话粘滞(session sticky)的负载均衡算法呢？**即在同一个客户端上，在一次会话中，所有请求都路由到同一个服务器上。

最简单的方法就是建立一个客户端IP与服务器编号的关系映射表，客户端发出请求先走映射表，查到服务器编号，再请求到服务器。简单的方法，有些弊端：
* 客户端很多时，映射表就很大，浪费内存空间
* 客户端上线/下线，服务器扩容/缩容，都会导致映射失效，维护映射表的成本很高

借助哈希算法，对客户端IP或者会话ID进行取哈希值，然后将哈希值与服务器列表的大小进行取模运算，得到的值就是应该被路由到的服务器编号。如此便可把同一个IP来的请求都路由到同一个服务器上。

#### (6)数据分片 ####

(1)**假如我们有1T的日志文件，里面记录了搜索关键词，需要快速统计出每个关键词被搜索的次数，该如何做？**

该问题有2个难度：
* 日志太大，无法在一台机器内存中处理
* 日志太大，处理时间会很长

借助哈希算法，可以先对数据进行分片，然后采用多台机器并行处理。假设用n台机器，从日志文件中，依次对关键词取哈希值，然后对n取模，得到的结果就是被分配的机器编号。这样哈希值相同的关键词就分配到了同一个机器上，每个机器分别计算关键词出现的次数，最后合并起来。这实际上也是MapReduce的基本设计思想。

(2)**如何快速判断图片是否在图库中？**

假如图库中有1亿张图片，采取哈希表的方式建立图片唯一标识与存储路径的关系，可能会超出单台机器的内存上限。同样采取分片的思想，准备n台机器，每台维护一部分哈希表。每次从图库中取一个图片计算唯一标识，然后与n取模，得到的值就是要分配的机器编号，最后将该图片的唯一标识与图片路径发往对应的服务器构建哈希表。当要判断图片是否存在时，同样的哈希取值，然后与n取模，得到机器编号k，最后去k的哈希表中查找。

**估算1亿张图片构建哈希表所需空间**

哈希表中每个数据单元包含2个信息，图片唯一标识(哈希值)与图片路径，假设通过MD5计算哈希值，长度为128比特，即16字节；文件路径最长256字节，取平均128字节；考虑可能存在哈希冲突，采用链表法则需要存储指针8字节，因此一个数据单元平均152字节。假设一台机器装填因子0.75，那么所需内存为1亿*152/0.75/1024/1024/1024=18GB。假设一台机器8GB内存，则至少需要3台机器。

#### (7)分布式存储 ####

面对海量的数据，为了提高数据的读写能力，一般采用分布式的方式存储数据，比如分布式缓存，而海量数据的分布式缓存通常需要多机部署。根据分片的思想，通过哈希算法对数据取哈希值，然后对机器取模，得到应该存储对机器编号。但是，如果数据增多，机器扩容，比如从10台扩容到11台，这就可能会出问题，比如12，原来对10取模，现在对11取模，结果不一样，那数据就分配错了，因此需要重新计算哈希值并且要转移数据到正确对机器上。这就相当于大量缓存突然失效，发送雪崩效应，压垮数据库。

所以我们需要一种方法，可以在加入一台机器后，并不需要做大量对数据转移，此时**一致性哈希算法**闪亮登场。比如表盘，现在有3台机器，分别是12点，4点，8点的位置，以顺时针的方向，数据1，2，3存到机器4中，5，7存在机器8中，9，10存在机器12中，现在要增加一台机器6，那么只需要把数据5转移到机器6中即可，其他的数据都不需要动。

# 13.二叉树

关于树的几个概念：
* 节点。父节点，子节点，兄弟节点，叶子节点
* 树的高度。根节点的高度
* 节点高度。节点到叶子节点的最长路径（边数）
* 节点深度。根节点到该节点的边数
* 节点层数。节点深度+1

深度指的是到根节点的边数，自上而下；高度指的是到叶节点的边数，自下而上。

### 1.二叉树 ###

树的结构有多种，常用的还是二叉树，每个节点最多两个子节点。
* 满二叉树。叶子节点全在最底层；除了叶子节点其他节点都有两个子节点
* 完全二叉树。叶子节点都在最下面两层；最底层的叶子节点靠左排列；不是最底层的其他层节点个数都达到最大。简单来说完全二叉树就是满二叉树从右向左去掉连续几个叶子节点。

**如何表示/存储二叉树？**

万变不离其宗，存储二叉树，有基于指针/引用的链式存储和基于数组的顺序存储两种方式。
* 常见的就是链式存储，每个节点有3个字段，1个数据域，1个左子节点指针，1个右子节点指针
* 基于数组存储，就是把树从自上而下，从左到右将节点数据存在数组中。需要注意的是根节点存储在数组下标i为1的位置，左子节点存储在下标为 2 * i = 2 的位置，右子节点存储在下标为 2 * i + 1 = 3 的位置。以此类推，如果节点下标为i，则左子子节点下标为 2 * i,右子节点下标为 2 * i + 1,父节点下标为 i/2

对于数组存储二叉树，如果某个节点缺失了，其对应的数组位置需要空出来，这样就会导致浪费内存。所以如果是完全二叉树，则用数组存储很节省内存，相比链式存储不需要额外存储指针。这就是为什么完全二叉树要求最底层叶子节点都靠左排列的原因。实际上，堆就是一种完全二叉树，常用的存储方式就是数组。

### 2.二叉树的遍历 ###

**二叉树遍历的时间复杂度是O(n)**,经典的3种遍历方法：
* 前序遍历。先打印节点，再打印左子树，最后打印右子树
* 中序遍历。先打印左子树，再打印节点，最后打印右子树
* 后序遍历。先打印左子树，再打印右子树，最后打印节点

所谓的前中后，指的是节点相比于左右子节点的遍历顺序。话不多说，最重要的也是最基本的，前中后序遍历代码如下：
```
def pre_order(tree):
    """
    二叉树前序遍历
    :param tree:
    :return:
    """
    if not tree:
        return
    print(tree.val, end=' ')
    pre_order(tree.left)
    pre_order(tree.right)


def in_order(tree):
    """
    二叉树中序遍历
    :param tree:
    :return:
    """
    if not tree:
        return
    in_order(tree.left)
    print(tree.val, end=' ')
    in_order(tree.right)


def post_order(tree):
    """
    二叉树后序遍历
    :param tree:
    :return:
    """
    if not tree:
        return
    post_order(tree.right)
    post_order(tree.left)
    print(tree.val, end=' ')
```

### 3.二叉查找树 ###

问：与哈希表相比，二叉查找树的优点有哪些呢？
答：哈希表的插入、删除和查找操作时间复杂度可以达到极高的O(1)，但是有其局限性：
* 哈希表中数据无序存储，而二叉查找树中序遍历在O(n)的时间复杂度内，即可输出有序序列
* 哈希表扩容耗时多，遇到哈希冲突时性能不稳定，而平衡二叉查找树性能很稳定在O(logn)
* 哈希表因为冲突的存在，实际查找速度并不一定就比O(logn)快，加上哈希函数的耗时，效率也不一定就比平衡二叉树效率高
* 哈希表构造复杂，需要考虑哈希函数、冲突解决办法、扩容缩容策略、装填因子选择等，平衡二叉树只需要考虑平衡性即可，且这个问题解决方案稳定成熟



二叉查找树最大的特点是，支持动态数据集合的快速插入、删除、查找操作。
**二叉查找树要求，在树中任意一个节点，其左子树中的每个节点值，都要小于该节点的值，而右子树节点的值都大于该值。**

**二叉查找树的查找**

先取根节点，如果等于查找值，则返回；如果比查找值大，则在左子树中递归查找；如果比查找值小，则在右子树中递归查找。

**二叉查找树的插入**

类似与查找，从根节点开始比较节点值与插入值大小，直到找到一个空子节点位置插入。

**二叉查找树的删除**

删除有3种情况：
* 待删除节点没有子节点。直接将父节点指向该节点的指针，置为null
* 待删除节点只有1个子节点。将父节点指向该节点的指针，指向该节点的子节点
* 待删除节点有2个子节点。查找到该节点右子树中的最小节点，然后替换掉该节点，最后删除最小节点

另外也可以采取标记删除的方式，并不真正删除节点。这样会使删除操作类似于查找，比较简单，但是会浪费内存空间。

**二叉查找树其他操作**

快速查找最大节点、最小节点、前驱节点和后继节点。二叉查找树有一个重要的特性，就是**二叉查找树中序遍历可以输出有序的数据序列，时间复杂度是O(n)**，非常高效。

二叉树的插入、删除、查找查找，都是基于查找，那么时间复杂度其实都是跟树高成正比，那么如何求一个n个节点的完全二叉树高度呢？借助于等比数列，可以算出来完全二叉树的高度小于logn。显然不平衡的二叉查找树，树高会增加，查找性能会降低，极端情况下退化成链表，时间复杂度为O(n)。**平衡二叉查找树**树高接近logn，其插入、删除和查找操作的时间复杂度也比较稳定是O(logn)。

对于二叉查找树，如果存在相同键值的情况，该如何存储呢？
* 每个节点不仅存储一个数据，通过链表和数组等数据结构，把相同值的数据都存在该节点上
* 节点仍然只存储一个数据，将相同值的节点插入该节点的右子树，即把后插入的数据当作大于该节点的值来处理

# 14.红黑树

二叉查找树是最常用的一种二叉树，支持快速插入、删除和查找，时间复杂度与树高成正比，理想情况下是O(logn)。不过在频繁的动态更新过程中，可能会出现树高过大的情况，进而导致操作效率降低。极端情况下退化为链表，时间复杂度退化为O(n)，因此需要一种平衡二叉查找树，保持动态更新时保持稳定的操作效率。

### 1.平衡二叉查找树 ###

**平衡二叉树**严格定义是，二叉树任意节点的左右子树高度差不能大于1。因此满二叉树、完全二叉树都是平衡二叉树，非完全二叉树也可能是平衡二叉树。**平衡二叉查找树**就是满足平衡二叉树和二叉查找树的特性。最先被发明的平衡二叉查找树是AVL树。但是很多平衡二叉查找树并没有严格遵循任一节点左右子树高度差不大于1的要求。比如**红黑树**，从根节点到各个叶子节点的最长路径，有可能会比最短路径大一倍。所以说平衡，就是让整棵树相对的平衡，不出现左右子树高度差过大的情况，这样就可以让树高更低一些，插入、删除和查找操作效率更高一些。

### 2.红黑树 ###

红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似 log2n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是 O(logn)。

因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用跳表来替代它。

红黑树是一种不严格的平衡二叉查找树，但却是平衡二叉查找树的典型代表，在工程中被广泛使用。**红黑树中的节点，一类被标记为红色，另一类被标记为黑色**，除此之外还需要满足如下要求：
* 根节点是黑色的
* 叶子节点都是黑色的空节点（null），即叶子节点不存数据
* 相邻节点不能同时为红色，即红色节点被黑色节点隔开
* 每个节点，从该节点到达它的可达叶子节点的所有路径，都包含相同数量的黑色节点

**AVL树**是一种高度平衡的二叉树，查找效率非常高，但是代价就是每次插入、删除都要做调整，更加复杂耗时，因此对于频繁插入、删除的数据，使用AVL树代价就过高了。红黑树只是近似平衡，所以维护平衡成本相比更低。

# 15.堆

问：**快排时间复杂度和堆排序一样，甚至堆排序的时间复杂度更加稳定，但是实践中快排的性能往往比堆排序好，为什么？**

答：第一个原因是堆排序的数据访问不如快排友好，快排的数据是顺序访问，而堆排序是跳序访问，这样对于CPU缓存是不友好的。第二个原因是相同数据场景下，堆排序交换数据次数多于快排。快排是基于比较的排序算法，因此交换次数不会超过数据的逆序度；堆排序建堆过程会打乱原本的数据顺序，导致有序度降低。

问：**有一个包含 10 亿个搜索关键词的日志文件，如何快速获取到 Top 10 最热门的搜索关键词呢？**

答：选用散列表。我们就顺序扫描这 10 亿个搜索关键词。当扫描到某个关键词时，我们去散列表中查询。如果存在，我们就将对应的次数加一；如果不存在，我们就将它插入到散列表，并记录次数为 1。以此类推，等遍历完这 10 亿个搜索关键词之后，散列表中就存储了不重复的搜索关键词以及出现的次数。然后，我们再根据前面讲的用堆求 Top K 的方法，建立一个大小为 10 的小顶堆，遍历散列表，依次取出每个搜索关键词及对应出现的次数，然后与堆顶的搜索关键词对比。如果出现次数比堆顶搜索关键词的次数多，那就删除堆顶的关键词，将这个出现次数更多的关键词加入到堆中。当遍历完整个散列表中的搜索关键词之后，堆中的搜索关键词就是出现次数最多的 Top 10 搜索关键词了。

10 亿的关键词还是很多的。我们假设 10 亿条搜索关键词中不重复的有 1 亿条，如果每个搜索关键词的平均长度是 50 个字节，那存储 1 亿个关键词起码需要 5GB 的内存空间，而散列表因为要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存空间就更多了。**利用哈希算法**，创建 10 个空文件 00，01，02，……，09。我们遍历这 10 亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同 10 取模，得到的结果就是这个搜索关键词应该被分到的文件编号。对这 10 亿个关键词分片之后，每个文件都只有 1 亿的关键词，去除掉重复的，可能就只有 1000 万个，每个关键词平均 50 个字节，所以总的大小就是 500MB。针对每个包含 1 亿条搜索关键词的文件，利用散列表和堆，分别求出 Top 10，然后把这个 10 个 Top 10 放在一块，然后取这 100 个关键词中，出现次数最多的 10 个关键词，这就是这 10 亿数据中的 Top 10 最频繁的搜索关键词了。

**堆是一种特殊的树，堆排序是堆这种数据结构最经典的应用。堆排序是一种原地排序，时间复杂度为O(nlogn)。**

堆有2点要求：
* 堆是一个完全二叉树
* 堆中每个节点的值都必须大于等于（或者小于等于）左右节点的值

如果堆顶元素值最大，则称为大顶堆；堆顶元素值最小，则为小顶堆。由于堆是完全二叉树，因此用数组来存储最合适。

**堆插入元素**，时间复杂度O(logn)，采用从下往上的堆化方法，首先将元素插入数组最后一位，然后同父节点值进行比较，如果比父节点值大，则与父节点交换，然后继续和父节点相比，直到小于父节点的值。

**删除堆顶元素**，时间复杂度O(logn)，采用自下而上的堆化方法，首先将数组最后一位元素放到堆顶，然后父子节点对比交换元素，直到满足堆的平衡。这样可以保证堆化的结果满足完全二叉树的特性，否则直接删除堆顶元素，然后子节点顶上去再进行父子节点比较交换，这样会出现数组空洞，不满足完全二叉树特性。

### 堆排序 ###

堆排序大致有2个步骤，建堆和排序

**1.建堆**
建堆的时间复杂度是O(n)。
首先将数组原地建成一个堆，不借助其他数组。建堆的过程有2种思路，第一种是从前往后处理数组数据，每个数据入堆时，都是从下往上堆化。第二种是从后向前处理数据，每个数据自上而下堆化，比子节点小的与子节点进行交换。对于第二种思路，可以直接从非叶子节点开始，因为叶子节点不能向下比较了。这样实际堆化的数据是从数组下标n/2到1的数据，n/2+1到n的数据都是叶子节点。

**2.排序**

建堆结束后，数组中的数据，前一部分是按照大顶堆的特性来组织的，第一个元素就是堆顶，最大的元素。将堆顶元素与最后一个元素交换，则最大元素就到了下标为n的位置。类似于删除堆顶元素的操作，不断的堆化，最大的元素依次放入数组第n、n-1、...的位置，当堆中只剩下标为1的1个元素，排序完成。

整个堆排序过程，都只需要个别临时存储空间，所以堆排序是原地排序算法。建堆时间复杂度为O(n),排序时间复杂度为O(nlogn)，整体时间复杂度是O(nlogn)。由于堆排序涉及交换堆顶和最后一个节点的操作，因此不是稳定的排序算法。

### 堆的重要应用 ###

**1.优先级队列**

队列的特性是先进先出，优先级队列则是优先级高的先出队。用堆来实现优先级队列是最高效、最直接的。一个堆就可以看作一个优先级队列，堆顶元素就是最高优先级的元素。

**（1）小顶堆合并有序小文件**

我们将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将 100 个小文件中的数据依次放入到大文件中。删除堆顶数据和往堆中插入数据的时间复杂度都是 O(logn)，n 表示堆中的数据个数，这里就是 100。相比数组的存储方式，高效了很多。

**（2）小顶堆高性能定时器**

假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如 1 秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。但是，这样每过 1 秒就扫描一遍任务列表的做法比较低效，主要原因有两点：第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。

用优先级队列来解决。按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔 T。定时器就可以设定在 T 秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。这样，定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。

**2.Top K**

求 Top K 的问题可以抽象成两类，一类是针对静态数据集合，也就是说数据集合事先确定，不会再变。另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。

先随机取出N个数中的K个数，将这N个数构造为小顶堆，那么堆顶的数肯定就是这K个数中最小的数了，然后再将剩下的N-K个数与堆顶进行比较，如果大于堆顶，那么说明该数有机会成为TopK，就更新堆顶为该数，此时由于小顶堆的性质可能被破坏，就还需要调整堆；否则说明这个数最多只能成为Top K+1 th，因此就不用管它。然后就将下一个数与当前堆顶的数作比较，根据大小关系如上面所述方法进行操作，知道N-K个数都遍历完，此时还在堆中的K个数就是TopK了。

针对动态数据求 Top K 就是实时 Top K,可以一直都维护一个 K 大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前 K 大数据，我们都可以里立刻返回给他。

遍历数组需要 O(n) 的时间复杂度，一次堆化操作需要 O(logK) 的时间复杂度，所以最坏情况下，n 个元素都入堆一次，所以时间复杂度就是 O(nlogK)。

**3.求中位数**

我们需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。这样**大顶堆中的堆顶元素就是我们要找的中位数**。约定如果n是偶数，从小到大排序，那前 n/2 个数据存储在大顶堆中，后 n/2 个数据存储在小顶堆中。如果n是奇数，大顶堆存储 n/2+1 个数据，小顶堆存储 n/2 个数据。**如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；如果新加入的数据大于等于小顶堆的堆顶元素，我们就将这个新数据插入到小顶堆。**

可能出现两个堆中的数据个数不符合前面约定的情况，我们可以从一个堆中不停地将堆顶元素移动到另一个堆来调整满足约定。插入数据因为需要涉及堆化，所以时间复杂度变成了 O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是 O(1)。

# 16.图

# 17.深度优先搜索

# 18.广度优先搜索

# 20.Trie树

# 21.AC自动机

# 22.贪心算法

# 23.分治算法

# 24.回溯算法

# 25.动态规划

# 26.拓扑排序

# 27.最短路径

# 28.B+树
