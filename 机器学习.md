深度学习是机器学习的一个特定分支，两种主要的统计学方法是频率派估计和贝叶斯推断。

机器学习算法，指的是能够从经验E中学习，对于任务T的性能度量P有所提升。

通常机器学习的任务指的是如何处理样本，而样本则是已经量化的特征集合。

常见的机器学习解决的任务：分类、回归、机器翻译、异常检测、合成与采样、去噪、密度估计或者说概率质量函数估计

**1.容量、欠拟合、过拟合**

**泛化**是指在非训练数据集上表现良好的能力

**欠拟合**是指模型不能得到足够低的训练误差

**过拟合**是指训练温差和测试误差之间的差距过大

**容量**通俗来讲是指拟合各种函数的能力，低容量可能导致欠拟合，高容量可能导致过拟合

**奥卡姆剃刀**原则是指，在同样能够解释已知观测现象的假设中，应该选择最简单的

**贝叶斯误差**从已知真实分布预测出现的误差

**没有免费的午餐**定理是指，对于机器学习算法来讲，没有最好，只有更好

**正则化**是指修改学习算法，降低泛化误差而非训练误差

欠拟合和过拟合也是决定机器学习算法效果好不好的两个因素/指标。
而容量和误差之间也具有典型的关系，即当欠拟合时，训练误差和泛化误差都很大，
当增加容量时，训练误差和泛化误差都减小，但是两种误差的间距在扩大，最终泛化误差到了最小处后开始增大，此时进入过拟合机制。

机器学习研究的目标不是找一个通用的或者绝对最好的学习算法，而是在我们关注的数据生成分布上效果最好的算法。

**2.超参数和验证集**

超参数不是通过学习出来的，而是开始指定的(尽管可以通过嵌套学习得出最优超参数)。而验证集则是用于挑选合适超参数的数据子集。

**对于小数据集，可以采用k-折交叉验证算法**，将数据集分成k个不重合的子集，测试误差估计为k次计算后的平均测试误差。
在第i次测试时，数据的第i个子集用于验证集。其他数据用于训练集。

**3.最大似然估计**

估计可以分为点估计、函数估计。最常用的估计准则是最大似然估计。
最大似然估计最吸引人的地方在于，当样本数趋近于无穷大时，就收敛率而言是最好的渐进估计。
在合适的条件下，最大似然估计具有一致性，意味着训练样本数目趋向于无穷大时，参数的最大似然估计会收敛到真实值。
基于一致性和统计效率的原因，最大似然通常是机器学习首选的估计方法。

**4.贝叶斯统计**

与最大似然相比，贝叶斯统计有两个重要区别：
* 最大似然使用θ的点估计，贝叶斯使用θ的全分布
* 贝叶斯先验分布，影响概率质量密度向参数空间中偏好先验的区域偏移。实践中，先验通常表现为偏好更简单、更光滑的模型。

**监督学习**


